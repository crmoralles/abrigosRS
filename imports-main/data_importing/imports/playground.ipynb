{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground space\n",
    "\n",
    "Espaço para testes, PoC e validação de hipóteses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importação de dependências\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "import json\n",
    "import math\n",
    "from fuzzywuzzy import process\n",
    "from selector import environment_selector\n",
    "from firestore_query_batch import query_batch\n",
    "from firestore_model_abrigado_legacy import AbrigadoLegacyEntity, MembroFamiliarLegacyEntity, DocumentoLegacyEntity, ResponsavelLegacyEntity\n",
    "from firestore_model_abrigo_legacy import AbrigoLegacyEntity\n",
    "from nome import normalize_nome\n",
    "from abrigo_dict import get_abrigo_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_selector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configura chaves e parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firebase Firestore Config\n",
    "base_path = os.getenv(\"SOSRS_BASEPATH\")\n",
    "sosrs_environment = os.getenv(\"SOSRS_ENVIRONMENT\")\n",
    "sosrs_firestore_keyfile = os.getenv(\"SOSRS_FIRESTORE_KEYFILE\")\n",
    "sosrs_firestore_path = os.path.join(base_path, sosrs_firestore_keyfile)\n",
    "\n",
    "# Airtable Config\n",
    "sosrs_airtable_api_key = os.getenv(\"SOSRS_AIRTABLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autenticação do notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização do Firebase\n",
    "cred = credentials.Certificate(sosrs_firestore_path)\n",
    "\n",
    "firebase_admin.initialize_app(cred)\n",
    "\n",
    "client_firestore = firestore.client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulação de documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adiciona um novo Abrigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrigo_entity = AbrigoLegacyEntity(\n",
    "    create_in=datetime.now(timezone.utc),\n",
    "    update_in=datetime.now(timezone.utc),\n",
    "    tipo=\"Abrigo\",\n",
    "    city=\"Porto Alegre\",\n",
    "    nome=\"Abrigo não informado na importação\",\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    doc_ref = client_firestore.collection(\"abrigo\").document()\n",
    "    abrigo_entity.set_id(doc_ref.id)\n",
    "    print(abrigo_entity.id)\n",
    "    doc_ref.set(abrigo_entity.to_dict())\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao enviar dados: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soma total de abrigados, incluindo grupo familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Referencia a coleção\n",
    "collection_ref = client_firestore.collection(\"abrigado\")\n",
    "documents = collection_ref.stream()\n",
    "\n",
    "# Contadores para nomes\n",
    "total_non_empty_names = 0\n",
    "# Itera sobre cada documento na coleção\n",
    "for document in documents:\n",
    "    data = document.to_dict()\n",
    "    # Conta o nome do AbrigadoLegacyEntity se não for nulo ou vazio\n",
    "    if \"nome\" in data and isinstance(data[\"nome\"], str) and data[\"nome\"].strip():\n",
    "        total_non_empty_names += 1\n",
    "    # Verifica se grupoFamiliar existe, não é None, é uma lista, e conta os nomes não nulos/vazios\n",
    "    if \"grupoFamiliar\" in data and isinstance(data[\"grupoFamiliar\"], list):\n",
    "        for membro in data[\"grupoFamiliar\"]:\n",
    "            if \"nome\" in membro and isinstance(membro[\"nome\"], str) and membro[\"nome\"].strip():\n",
    "                total_non_empty_names += 1\n",
    "\n",
    "print(f\"Total de nomes não nulos ou vazios: {total_non_empty_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta abrigos por nome (like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_ref = client_firestore.collection('abrigo')\n",
    "\n",
    "search_text = 'enjoy'\n",
    "start_at = f'{search_text}'\n",
    "end_at = f'{search_text}\\uf8ff'\n",
    "\n",
    "query = collection_ref.where('nome', '>=', start_at).where('nome', '<=', end_at)\n",
    "results = query.stream()\n",
    "\n",
    "for doc in results:\n",
    "    doc_dict = doc.to_dict()\n",
    "    print(f'id: {doc.id}, nome: {doc_dict.get(\"nome\", \"Nome não encontrado\")}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta abrigos por nome (equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_ref = client_firestore.collection('abrigado95')\n",
    "\n",
    "search_text = '31KJeGHcEMeumEcnLFoF'\n",
    "\n",
    "query = collection_ref.where('id', '==', search_text)\n",
    "results = query.stream()\n",
    "\n",
    "for doc in results:\n",
    "    doc_dict = doc.to_dict()\n",
    "    print(doc_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta abrigos por nome (fuzzy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_text = \"castelo\"\n",
    "\n",
    "collection_ref = client_firestore.collection(\"abrigo\")\n",
    "docs = collection_ref.stream()\n",
    "\n",
    "all_docs = {}\n",
    "for doc in docs:\n",
    "    doc_dict = doc.to_dict()\n",
    "    nome = doc_dict.get(\"nome\")\n",
    "    if nome:\n",
    "        # Split words and remove duplicates\n",
    "        words = set(nome.split())\n",
    "        for word in words:\n",
    "            all_docs[word] = { doc.id, nome }\n",
    "\n",
    "nome_similar, score = process.extractOne(search_text, list(all_docs.keys()))\n",
    "if score >= 80:\n",
    "    similar_doc_id = all_docs[nome_similar]\n",
    "    print(f\"nome similar: {nome_similar}, score: {score}, nome: {all_docs[nome_similar]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de dados em massa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajusta dados via arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasource_file = \"./pipeline/.working/ajuste_nome_sem_abrigo.xlsx\"\n",
    "    collection_name = \"abrigado\"\n",
    "\n",
    "    xlsx = pd.ExcelFile(datasource_file)\n",
    "    data_source = pd.read_excel(xlsx, header=0, names=[\"abrigo_nome\", \"abrigado_nome\"], usecols=range(2))\n",
    "    data_source = data_source[data_source[\"abrigado_nome\"].notna() & (data_source[\"abrigado_nome\"] != \"\")]\n",
    "    data_source[\"search_field_name\"] = data_source.apply(lambda row: normalize_nome(row[\"abrigado_nome\"]), axis=1)\n",
    "    registers = data_source.to_dict(orient=\"records\")\n",
    "\n",
    "    batch = client_firestore.batch()\n",
    "    count = 0\n",
    "\n",
    "    for value in registers:\n",
    "        abrigo_nome, abrigo_id = get_abrigo_info(value[\"abrigo_nome\"])\n",
    "        \n",
    "        if abrigo_id is not None:\n",
    "            update_dict = {\n",
    "                \"abrigoId\": abrigo_id,\n",
    "                \"abrigoNome\": abrigo_nome,\n",
    "            }\n",
    "\n",
    "            collection_ref = client_firestore.collection(collection_name)\n",
    "            query = collection_ref.where(\"search_field_name\", \"==\", value[\"search_field_name\"])\n",
    "            results = query.stream()\n",
    "            for doc in results:\n",
    "                doc_ref = client_firestore.collection(collection_name).document(doc.id)\n",
    "                batch.update(doc_ref, update_dict)\n",
    "                count += 1\n",
    "                if count % 500 == 0:\n",
    "                    batch.commit()\n",
    "                    print(f\"Batch com {count} operações enviado.\")\n",
    "                    batch = client_firestore.batch()\n",
    "                    count = 0\n",
    "\n",
    "    if count > 0:\n",
    "        batch.commit()\n",
    "        print(f\"Batch final com {count} operações enviado.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error updating Firestore documents: {e}\")\n",
    "    collection_name = \"abrigado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusão de Abrigos e seus abrigados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização do Firebase\n",
    "abrigo_collection_name = \"abrigo\"\n",
    "abrigado_collection_name = \"abrigado\"\n",
    "\n",
    "# abrigoId que será manipulado\n",
    "abrigo_id = \"JtcFtoURWfjlSmcChOVo\"\n",
    "abrigo_field_id = \"abrigoId\"\n",
    "\n",
    "\n",
    "# Executa a exclusão de todos os abrigados de um abrigo\n",
    "print(f\"Excluindo todos os abrigados do abrigo {abrigo_id}\")\n",
    "\n",
    "\n",
    "# Query\n",
    "collection_abrigado_ref = client_firestore.collection(abrigado_collection_name)\n",
    "pre_delete_query = collection_abrigado_ref.where(abrigo_field_id, \"==\", abrigo_id)\n",
    "results = pre_delete_query.stream()\n",
    "\n",
    "# Contar documentos antes da exclusão\n",
    "pre_delete_count = sum(1 for _ in pre_delete_query.stream())\n",
    "print(f\"Total de documentos a serem deletados: {pre_delete_count}\")\n",
    "\n",
    "# Processar a deleção em lotes\n",
    "results = pre_delete_query.stream()\n",
    "deleted_count = 0\n",
    "batch = client_firestore.batch()\n",
    "batch_count = 0\n",
    "\n",
    "# Cria lote de exclusão\n",
    "for doc in results:\n",
    "    doc_ref = collection_abrigado_ref.document(doc.id)\n",
    "    batch.delete(doc_ref)\n",
    "    batch_count += 1\n",
    "    deleted_count += 1\n",
    "\n",
    "    # Se o lote atingir o limite de 500, fazer commit e iniciar um novo lote\n",
    "    if batch_count >= 500:\n",
    "        batch.commit()\n",
    "        batch = client_firestore.batch()\n",
    "        batch_count = 0\n",
    "\n",
    "# Commit do último lote se houver documentos restantes\n",
    "if batch_count > 0:\n",
    "    batch.commit()\n",
    "\n",
    "print(f\"Total de documentos deletados: {deleted_count}\")\n",
    "\n",
    "# Exclusão do abrigo\n",
    "print(f\"Excluindo abrigo {abrigo_id}\")\n",
    "abrigo_ref = client_firestore.collection(abrigo_collection_name).document(abrigo_id)\n",
    "abrigo_ref.delete()\n",
    "print(f\"Abrigo {abrigo_id} excluído com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correção dos abrigados em abrigo \"Em nome\"\n",
    "O objetivo desse bloco é corrigr as importações que tiveram problemas com o nome do abrigo quando ele estva em branço. Em um dos processos de importação os abrigos foram marcados com \"Em abrigo\" ao invés de \"Sem abrigo\". Esse processo processa uma planilha xlsx de ajustes que contém o nome do abrigo e nome do abrigado. A planilha é iterada, pesquisando pelo abrigado. Quando um abrigado é encontrado então é verificado se o nome do abrigo no qual ele está registrado é `Em abrigo`. Quando essa condição é satisfeita então ele é atualizado para o abrigo `Abrigo não informado na importação`, ou seja, todas as importações que não tiveram abrigos são corrigidas para esse novo abrigo ao invés de ficarem em um estado inválido, uma vez que o abrigo `Em abrigo ` não existe.\n",
    "\n",
    "> Existe chance de colisão de nomes, ou seja, pessoas diferentes com o mesmo nome em função da falta de informações obrigatórias (atualmente somente o nome). A correção trabalha de forma otimista, ou seja, considera que a colisão de nomes não ocorrerá ou terá um incidência muito baixa.\n",
    "\n",
    "> **Modelo do xlsx:**\n",
    ">\n",
    "> Coluna A=Abrigo, B=Abrigado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    datasource_file = \"./pipeline/.working/ajuste_nome_sem_abrigo.xlsx\"\n",
    "    collection_name = \"abrigado\"\n",
    "    wrong_name = [\"Em abrigo\", \"em abrigo\"]\n",
    "\n",
    "    # Faz leitura do arquivo de ajuste (Coluna A=Abrigo, B=Abrigado)\n",
    "    xlsx = pd.ExcelFile(datasource_file)\n",
    "    data_source = pd.read_excel(xlsx, header=0, names=[\"abrigo_nome\", \"abrigado_nome\"], usecols=range(2))\n",
    "    data_source = data_source[data_source[\"abrigado_nome\"].notna() & (data_source[\"abrigado_nome\"] != \"\")]\n",
    "    data_source[\"search_field_name\"] = data_source.apply(lambda row: normalize_nome(row[\"abrigado_nome\"]), axis=1)\n",
    "    registers = data_source.to_dict(orient=\"records\")\n",
    "\n",
    "    # Inicia batch de atualização\n",
    "    batch = client_firestore.batch()\n",
    "    count = 0\n",
    "\n",
    "    # Registra casos onde o abrigado foi visto como sem abrigo informado mas foi registrado em outro abrigo posterirmente\n",
    "    count_abrigados_atualizados_em_outras_fontes = 0\n",
    "\n",
    "    # Itera sobre os registros do arquivo de ajuste\n",
    "    for value in registers:\n",
    "        abrigo_nome, abrigo_id = get_abrigo_info(value[\"abrigo_nome\"])\n",
    "\n",
    "        if abrigo_id is not None:\n",
    "\n",
    "            # Cria objeto de atualização\n",
    "            update_dict = {\n",
    "                \"abrigoId\": abrigo_id,\n",
    "                \"abrigoNome\": abrigo_nome,\n",
    "            }\n",
    "\n",
    "            # Query para buscar os documentos a serem atualizados\n",
    "            collection_ref = client_firestore.collection(collection_name)\n",
    "            query = collection_ref.where(\"search_field_name\", \"==\", value[\"search_field_name\"])\n",
    "            results = query.stream()\n",
    "\n",
    "            # Itera sobre os documentos de abrigados encontrados com o filtro search_field_name\n",
    "            for doc in results:\n",
    "\n",
    "                # Somente atualiza documentos que possuem o nome do abrigo incorreto\n",
    "                abrigado = doc.to_dict()\n",
    "                if abrigado.get(\"abrigoNome\") in wrong_name:\n",
    "                    doc_ref = client_firestore.collection(collection_name).document(doc.id)\n",
    "                    batch.update(doc_ref, update_dict)\n",
    "                    count += 1\n",
    "\n",
    "                    # Se o batch atingir 500 operações, faz commit e inicia um novo batch\n",
    "                    if count % 500 == 0:\n",
    "                        batch.commit()\n",
    "                        print(f\"Batch com {count} operações enviado.\")\n",
    "                        batch = client_firestore.batch()\n",
    "                        count = 0\n",
    "                else:\n",
    "                    print(f\"Abrigado {abrigado['nome']} já está em outro abrigo ({abrigado['abrigoNome']}). Ignorando atualização.\")\n",
    "                    count_abrigados_atualizados_em_outras_fontes += 1\n",
    "\n",
    "    # Commit do último batch se houver operações restantes\n",
    "    if count > 0:\n",
    "        batch.commit()\n",
    "        print(f\"Batch final com {count} operações enviado.\")\n",
    "        print(f\"Total de abrigados atualizados em outras fontes: {count_abrigados_atualizados_em_outras_fontes}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error updating Firestore documents: {e}\")\n",
    "    collection_name = \"abrigado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanitização de collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanitiza dados de abrigados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"abrigado\"\n",
    "\n",
    "def extract_cpf(nome: str) -> Optional[str]:\n",
    "    if nome is None:\n",
    "        return None\n",
    "    \n",
    "    match = re.search(r\"CPF (\\d{11})\", nome)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def format_date_if_necessary(date):\n",
    "    if pd.isna(date):\n",
    "        return datetime.now().strftime(\"%b %d, %Y at %I:%M:%S %p UTC-3\")\n",
    "\n",
    "    if isinstance(date, datetime):\n",
    "        return date\n",
    "\n",
    "    try:\n",
    "        parsed_date = datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        return parsed_date.strftime(\"%b %d, %Y at %I:%M:%S %p UTC-3\")\n",
    "    except ValueError:\n",
    "        return datetime.now().strftime(\"%b %d, %Y at %I:%M:%S %p UTC-3\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Prepara stream de documentos\n",
    "    abrigos_ref = client_firestore.collection(collection_name)\n",
    "    docs = abrigos_ref.stream()\n",
    "\n",
    "    # Prepara o batch para atualizações\n",
    "    batch = client_firestore.batch()\n",
    "    count = 0\n",
    "    batch_size = 500\n",
    "\n",
    "    for doc in docs:\n",
    "        user_data = doc.to_dict()\n",
    "\n",
    "        # Referência ao documento que será atualizado\n",
    "        doc_ref = abrigos_ref.document(doc.id)\n",
    "\n",
    "        # Normaliza nome e cria search_field\n",
    "        nome = user_data.get(\"nome\")\n",
    "        search_field_name = normalize_nome(nome)\n",
    "        batch.update(doc_ref, {\"search_field_name\": search_field_name})\n",
    "\n",
    "        # Tenta extrair o CPF do nome\n",
    "        cpf = extract_cpf(nome)\n",
    "        documentos = user_data.get(\"documentos\", [])\n",
    "        if cpf not in documentos and cpf is not None:\n",
    "            updated_documentos = documentos + [cpf]\n",
    "            batch.update(doc_ref, {\"documentos\": updated_documentos})\n",
    "\n",
    "        # Ajusta o campo id para o valor do documento caso esteja vazio\n",
    "        id = user_data.get(\"id\")\n",
    "        if not id:\n",
    "            batch.update(doc_ref, {\"id\": doc.id})\n",
    "\n",
    "        # Ajusta create_in\n",
    "        create_in = user_data.get(\"create_in\")\n",
    "        create_in = format_date_if_necessary(create_in)\n",
    "        batch.update(doc_ref, {\"create_in\": create_in})\n",
    "\n",
    "        # Ajusta update_in\n",
    "        update_in = user_data.get(\"update_in\")\n",
    "        update_in = format_date_if_necessary(update_in)\n",
    "        batch.update(doc_ref, {\"update_in\": update_in})\n",
    "\n",
    "        # Remove a propriedades inseridas incorretamente durante as cargas\n",
    "        batch.update(doc_ref, {\"created_in\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"nome_normalizado\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"search_field\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"documento\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"cpf\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"codCadUnico\": firestore.DELETE_FIELD})\n",
    "\n",
    "        # Incrementa o contador de operações no batch\n",
    "        count += 1\n",
    "\n",
    "        # Se o batch atingir o tamanho máximo, comitar e começar um novo batch\n",
    "        if count >= batch_size:\n",
    "            batch.commit()\n",
    "            batch = client_firestore.batch()\n",
    "            count = 0\n",
    "\n",
    "    # Comitar qualquer operação restante no batch final\n",
    "    if count > 0:\n",
    "        batch.commit()\n",
    "\n",
    "    print(\"Atualização em batch concluída.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro na atualização em batch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanitiza dados de Abrigos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"abrigo\"\n",
    "\n",
    "\n",
    "try:\n",
    "    # Prepara stream de documentos\n",
    "    abrigos_ref = client_firestore.collection(collection_name)\n",
    "    docs = abrigos_ref.stream()\n",
    "\n",
    "    # Prepara o batch para atualizações\n",
    "    batch = client_firestore.batch()\n",
    "    count = 0\n",
    "    batch_size = 500\n",
    "\n",
    "    for doc in docs:\n",
    "        user_data = doc.to_dict()\n",
    "\n",
    "        # Referência ao documento que será atualizado\n",
    "        doc_ref = abrigos_ref.document(doc.id)\n",
    "\n",
    "        # Remove NaN de vagas e seta para null se for o caso\n",
    "        vagas = user_data.get(\"vagas\")\n",
    "        if vagas is not None and (isinstance(vagas, float) and math.isnan(vagas)):\n",
    "            vagas = None\n",
    "            batch.update(doc_ref, {\"vagas\": None})\n",
    "            count += 1\n",
    "\n",
    "        # Seta None quando vaga for uma string vazia\n",
    "        if vagas == \"\":\n",
    "            vagas = None\n",
    "            batch.update(doc_ref, {\"vagas\": None})\n",
    "            count += 1\n",
    "\n",
    "        # Converte 'vagas' para inteiro se for uma string numérica\n",
    "        if isinstance(vagas, str) and vagas.isdigit():\n",
    "            batch.update(doc_ref, {\"vagas\": int(vagas)})\n",
    "            count += 1\n",
    "\n",
    "        # Remove NaN de vagas_ocupadas e seta para null se for o caso\n",
    "        vagas_ocupadas = user_data.get(\"vagas_ocupadas\")\n",
    "        if vagas_ocupadas is not None and (isinstance(vagas_ocupadas, float) and math.isnan(vagas_ocupadas)):\n",
    "            vagas_ocupadas = None\n",
    "            batch.update(doc_ref, {\"vagas_ocupadas\": None})\n",
    "            count += 1\n",
    "\n",
    "        # Seta None quando vagas_ocupadas for uma string vazia\n",
    "        if vagas_ocupadas == \"\":\n",
    "            vagas_ocupadas = None\n",
    "            batch.update(doc_ref, {\"vagas_ocupadas\": None})\n",
    "            count += 1\n",
    "\n",
    "        # Converte 'vagas_ocupadas' para inteiro se for uma string numérica\n",
    "        if isinstance(vagas_ocupadas, str) and vagas_ocupadas.isdigit():\n",
    "            batch.update(doc_ref, {\"vagas_ocupadas\": int(vagas_ocupadas)})\n",
    "            count += 1\n",
    "\n",
    "        # Ajusta telefone persisitdo como array\n",
    "        telefone = user_data.get(\"telefone\")\n",
    "        if isinstance(telefone, list) and len(telefone) >= 1:\n",
    "            batch.update(doc_ref, {\"telefone\": telefone[0]})\n",
    "            count += 1\n",
    "            \n",
    "        # Ajusta nome persisitido como array\n",
    "        nome = user_data.get(\"nome\")\n",
    "        if isinstance(nome, list) and len(nome) >= 1:\n",
    "            batch.update(doc_ref, {\"nome\": nome[0]})\n",
    "            count += 1    \n",
    "            \n",
    "        # Ajusta city persisitido como array\n",
    "        city = user_data.get(\"city\")\n",
    "        if isinstance(city, list) and len(city) >= 1:\n",
    "            batch.update(doc_ref, {\"city\": city[0]})\n",
    "            count += 1                \n",
    "\n",
    "        # Se o batch atingir o tamanho máximo, comitar e começar um novo batch\n",
    "        if count >= batch_size:\n",
    "            batch.commit()\n",
    "            batch = client_firestore.batch()\n",
    "            count = 0\n",
    "\n",
    "    # Comitar qualquer operação restante no batch final\n",
    "    if count > 0:\n",
    "        batch.commit()\n",
    "\n",
    "    print(\"Atualização em batch concluída.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro na atualização em batch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanitização de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"abrigado\"\n",
    "\n",
    "\n",
    "def extract_cpf(nome: str) -> Optional[str]:\n",
    "    if nome is None:\n",
    "        return None\n",
    "\n",
    "    # CPF Patterns\n",
    "    cpf_patterns = \"(?:CPF)?[^\\d]?([\\(\\[\\{]?(\\d{3}[\\.\\-\\s,]?){3}\\d{2}[\\)\\]\\}]?|\\d{11})\"\n",
    "\n",
    "    match = re.search(cpf_patterns)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def format_date_if_necessary(date):\n",
    "    if pd.isna(date):\n",
    "        return datetime.now().strftime(\"%b %d, %Y at %I:%M:%S %p UTC-3\")\n",
    "\n",
    "    if isinstance(date, datetime):\n",
    "        return date\n",
    "\n",
    "    try:\n",
    "        parsed_date = datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "        return parsed_date.strftime(\"%b %d, %Y at %I:%M:%S %p UTC-3\")\n",
    "    except ValueError:\n",
    "        return datetime.now().strftime(\"%b %d, %Y at %I:%M:%S %p UTC-3\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Prepara stream de documentos\n",
    "    abrigos_ref = client_firestore.collection(collection_name)\n",
    "    docs = abrigos_ref.stream()\n",
    "\n",
    "    # Prepara o batch para atualizações\n",
    "    batch = client_firestore.batch()\n",
    "    count = 0\n",
    "    batch_size = 500\n",
    "\n",
    "    for doc in docs:\n",
    "        user_data = doc.to_dict()\n",
    "\n",
    "        # Referência ao documento que será atualizado\n",
    "        doc_ref = abrigos_ref.document(doc.id)\n",
    "\n",
    "        # Normaliza nome e cria search_field\n",
    "        nome = user_data.get(\"nome\")\n",
    "        search_field_name = normalize_nome(nome)\n",
    "        batch.update(doc_ref, {\"search_field_name\": search_field_name})\n",
    "\n",
    "        # Tenta extrair o CPF do nome\n",
    "        cpf = extract_cpf(nome)\n",
    "        documentos = user_data.get(\"documentos\", [])\n",
    "        if cpf not in documentos:\n",
    "            updated_documentos = documentos + [cpf]\n",
    "            batch.update(doc_ref, {\"documentos\": updated_documentos})\n",
    "\n",
    "        # Ajusta o campo id para o valor do documento caso esteja vazio\n",
    "        id = user_data.get(\"id\")\n",
    "        if not id:\n",
    "            batch.update(doc_ref, {\"id\": doc.id})\n",
    "\n",
    "        # Ajusta create_in\n",
    "        create_in = user_data.get(\"create_in\")\n",
    "        create_in = format_date_if_necessary(create_in)\n",
    "        batch.update(doc_ref, {\"create_in\": create_in})\n",
    "\n",
    "        # Ajusta update_in\n",
    "        update_in = user_data.get(\"update_in\")\n",
    "        update_in = format_date_if_necessary(update_in)\n",
    "        batch.update(doc_ref, {\"update_in\": update_in})\n",
    "\n",
    "        # Remove a propriedades inseridas incorretamente durante as cargas\n",
    "        batch.update(doc_ref, {\"created_in\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"nome_normalizado\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"search_field\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"documento\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"cpf\": firestore.DELETE_FIELD})\n",
    "        batch.update(doc_ref, {\"codCadUnico\": firestore.DELETE_FIELD})\n",
    "\n",
    "        # Incrementa o contador de operações no batch\n",
    "        count += 1\n",
    "\n",
    "        # Se o batch atingir o tamanho máximo, comitar e começar um novo batch\n",
    "        if count >= batch_size:\n",
    "            batch.commit()\n",
    "            batch = client_firestore.batch()\n",
    "            count = 0\n",
    "\n",
    "    # Comitar qualquer operação restante no batch final\n",
    "    if count > 0:\n",
    "        batch.commit()\n",
    "\n",
    "    print(\"Atualização em batch concluída.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro na atualização em batch: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contagem de documentos por tipos de estrutura de documento encontrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrigo_ref = client_firestore.collection('abrigo')\n",
    "\n",
    "# Dicionário para armazenar as estruturas encontradas e suas contagens\n",
    "estruturas = {}\n",
    "# Dicionário para rastrear tipos diferentes para a mesma propriedade, excluindo NoneType\n",
    "tipos_por_propriedade = {}\n",
    "\n",
    "# Itera sobre os documentos na coleção\n",
    "for doc in abrigo_ref.stream():\n",
    "    doc_data = doc.to_dict()\n",
    "    # Cria uma representação da estrutura do documento\n",
    "    estrutura = {}\n",
    "    for key, value in doc_data.items():\n",
    "        tipo = type(value).__name__\n",
    "        if tipo != 'NoneType':  # Ignora NoneType\n",
    "            estrutura[key] = tipo\n",
    "\n",
    "            # Rastrear os tipos de cada propriedade, excluindo NoneType\n",
    "            if key in tipos_por_propriedade:\n",
    "                tipos_por_propriedade[key].add(tipo)\n",
    "            else:\n",
    "                tipos_por_propriedade[key] = {tipo}\n",
    "\n",
    "    # Converte a estrutura em uma string para usá-la como chave no dicionário\n",
    "    estrutura_str = str(sorted(estrutura.items()))\n",
    "\n",
    "    # Atualiza a contagem de documentos com essa estrutura\n",
    "    if estrutura_str in estruturas:\n",
    "        estruturas[estrutura_str] += 1\n",
    "    else:\n",
    "        estruturas[estrutura_str] = 1\n",
    "\n",
    "# Imprime a quantidade total de documentos por cada estrutura, e a contagem primeiro\n",
    "for estrutura, count in estruturas.items():\n",
    "    print(f\"Contagem: {count}, Estrutura: {estrutura}\")\n",
    "\n",
    "# Imprime os diferentes tipos encontrados para cada propriedade, excluindo NoneType e propriedades com apenas um tipo\n",
    "print(\"\\nTipos diferentes encontrados por propriedade (excluindo NoneType e propriedades com um único tipo):\")\n",
    "for prop, tipos in tipos_por_propriedade.items():\n",
    "    if len(tipos) > 1:  # Exclui propriedades com apenas um tipo\n",
    "        print(f\"Propriedade: {prop}, Tipos: {tipos}, Contagem de Tipos: {len(tipos)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backps e exportações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup de collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o nome da coleção e o arquivo de saída\n",
    "collection_name = 'users'\n",
    "output_file = '../temp/collection_users.json'\n",
    "output_file_csv = '../temp/collection_users.csv'\n",
    "\n",
    "def datetime_converter(o):\n",
    "    if isinstance(o, firestore.datetime.datetime):\n",
    "        return o.isoformat()\n",
    "    raise TypeError(\"Object of type 'DatetimeWithNanoseconds' is not JSON serializable\")\n",
    "\n",
    "def backup_collection_to_json(collection_name, output_file):\n",
    "    try:\n",
    "        collection_ref = client_firestore.collection(collection_name)\n",
    "        docs = collection_ref.stream()\n",
    "        \n",
    "        # Criar uma lista para armazenar todos os documentos como dicionários\n",
    "        all_docs = []\n",
    "        for doc in docs:\n",
    "            doc_dict = doc.to_dict()\n",
    "            # Converter todos os DatetimeWithNanoseconds para strings ISO 8601\n",
    "            for key, value in doc_dict.items():\n",
    "                if isinstance(value, datetime):\n",
    "                    doc_dict[key] = value.isoformat()\n",
    "            all_docs.append(doc_dict)\n",
    "        \n",
    "        # Escrever os dados em um arquivo JSON\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(all_docs, f, default=datetime_converter, ensure_ascii=False, indent=4)\n",
    "            \n",
    "        # Gera csv\n",
    "        df = pd.read_json(output_file)\n",
    "\n",
    "        # Exporta o DataFrame para um arquivo CSV\n",
    "        df.to_csv(output_file_csv, index=False)    \n",
    "        \n",
    "        print(f\"Backup da coleção '{collection_name}' completo. Dados salvos em '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante o backup: {e}\")\n",
    "\n",
    "# Executar o backup\n",
    "backup_collection_to_json(collection_name, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportação de abrigados para orgãos de segurança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o nome da coleção e o arquivo de saída\n",
    "collection_name = \"abrigado\"\n",
    "output_file = \"../temp/abrigado_export.xlsx\"\n",
    "\n",
    "\n",
    "def datetime_converter(o):\n",
    "    if isinstance(o, datetime):\n",
    "        return o.__str__()\n",
    "\n",
    "\n",
    "def load_collection(collection_name):\n",
    "    try:\n",
    "        collection_ref = client_firestore.collection(collection_name)\n",
    "        docs = collection_ref.stream()\n",
    "\n",
    "        all_docs = []\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                doc_dict = doc.to_dict()\n",
    "                for key, value in doc_dict.items():\n",
    "                    if isinstance(value, datetime):\n",
    "                        doc_dict[key] = value.isoformat()\n",
    "                all_docs.append(\n",
    "                    doc_dict\n",
    "                )  # Certifique-se que esta linha está dentro do loop\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar documento {doc.id}: {e}\")\n",
    "\n",
    "        if not all_docs:  # Checa se a lista está vazia\n",
    "            print(\"Nenhum documento foi carregado da coleção.\")\n",
    "        return all_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar coleção: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def export_to_excel(data, output_file):\n",
    "    try:\n",
    "        # Criando um DataFrame a partir dos dados\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if df.empty:  # Verifica se o DataFrame está vazio\n",
    "            print(\"O DataFrame está vazio. Nenhum dado para exportar.\")\n",
    "        else:\n",
    "            # Salvando o DataFrame em um arquivo Excel\n",
    "            df.to_excel(output_file, index=False)\n",
    "            print(f\"Dados exportados para {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar arquivo Excel: {e}\")\n",
    "\n",
    "\n",
    "# Carregando dados da coleção\n",
    "data = load_collection(collection_name)\n",
    "\n",
    "# Exportando dados para o arquivo Excel\n",
    "export_to_excel(data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportação de abrigos para orgãos de segurança"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o nome da coleção e o arquivo de saída\n",
    "collection_name = \"abrigo\"\n",
    "output_file = \"../temp/abrigo_export.xlsx\"\n",
    "\n",
    "\n",
    "def datetime_converter(o):\n",
    "    if isinstance(o, datetime):\n",
    "        return o.__str__()\n",
    "\n",
    "\n",
    "def load_collection(collection_name):\n",
    "    try:\n",
    "        collection_ref = client_firestore.collection(collection_name)\n",
    "        docs = collection_ref.stream()\n",
    "\n",
    "        all_docs = []\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                doc_dict = doc.to_dict()\n",
    "                for key, value in doc_dict.items():\n",
    "                    if isinstance(value, datetime):\n",
    "                        doc_dict[key] = value.isoformat()\n",
    "                all_docs.append(\n",
    "                    doc_dict\n",
    "                )  # Certifique-se que esta linha está dentro do loop\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar documento {doc.id}: {e}\")\n",
    "\n",
    "        if not all_docs:  # Checa se a lista está vazia\n",
    "            print(\"Nenhum documento foi carregado da coleção.\")\n",
    "        return all_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar coleção: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def export_to_excel(data, output_file):\n",
    "    try:\n",
    "        # Criando um DataFrame a partir dos dados\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        if df.empty:  # Verifica se o DataFrame está vazio\n",
    "            print(\"O DataFrame está vazio. Nenhum dado para exportar.\")\n",
    "        else:\n",
    "            # Salvando o DataFrame em um arquivo Excel\n",
    "            df.to_excel(output_file, index=False)\n",
    "            print(f\"Dados exportados para {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar arquivo Excel: {e}\")\n",
    "\n",
    "\n",
    "# Carregando dados da coleção\n",
    "data = load_collection(collection_name)\n",
    "\n",
    "# Exportando dados para o arquivo Excel\n",
    "export_to_excel(data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercício de integração com Airtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyairtable import Table\n",
    "\n",
    "api_key = sosrs_airtable_api_key\n",
    "base_id = 'appfQT2vfWc6sZRAx'\n",
    "table_name = 'shrH5MuXAGjBqqW0j/tblJ5UvIdVmYeTb8s'\n",
    "\n",
    "table = Table(api_key, base_id, table_name)\n",
    "\n",
    "# Buscar todos os registros\n",
    "records = table.all()\n",
    "\n",
    "# Exibir registros\n",
    "for record in records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios de integração como Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "\n",
    "# Define o escopo da API\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Faz a autenticação usando o arquivo de credenciais\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(sosrs_firestore_path, scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Define a planilha\n",
    "spreadsheet_id = \"1JrnFKHELLMVpYdsnndjIWa8zTHaG0B62ESV4JHWwc2w\"\n",
    "\n",
    "# Abre a planilha pelo nome e seleciona a aba específica\n",
    "sheet = client.open(spreadsheet_id).worksheet(\"Respostas ao formulário 1\")  # Substitua 'master_data_sheet' pela aba desejada\n",
    "\n",
    "# Obtém todos os dados da planilha em formato de lista de listas\n",
    "data = sheet.get_all_values()\n",
    "print(data)\n",
    "\n",
    "# Converte para DataFrame\n",
    "data_source = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "# Se você deseja usar nomes de colunas específicos e selecionar certas colunas\n",
    "data_source.columns = column_names  # Assegure-se de que 'column_names' tem o mesmo número de colunas que você está lendo\n",
    "data_source = data_source.iloc[:, :number_of_columns]  # Seleciona um número específico de colunas\n",
    "\n",
    "# Preenche valores NA com strings vazias\n",
    "data_source.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversão de PDF em Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 página por aba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Caminho do arquivo PDF\n",
    "file_path = r\"E:\\thiagoborba.me\\sosrs\\data_management\\data_importing\\temp\\abrigados_bom_conselho.pdf\"\n",
    "\n",
    "# Converter PDF para DataFrame\n",
    "# O parâmetro pages especifica as páginas a serem convertidas, 'all' para todas as páginas\n",
    "dfs = tabula.read_pdf(file_path, pages=\"all\", multiple_tables=True)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Salvar cada tabela como uma planilha no arquivo Excel\n",
    "writer = pd.ExcelWriter(r\"E:\\thiagoborba.me\\sosrs\\data_management\\data_importing\\temp\\abrigados_bom_conselho.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "if dfs:\n",
    "    for i, df in enumerate(dfs):\n",
    "        df.to_excel(writer, sheet_name=f'Sheet{i+1}')\n",
    "    print(\"A conversão foi concluída com sucesso!\")\n",
    "else:\n",
    "    print(\"Nenhuma tabela encontrada no PDF.\")\n",
    "writer.close()\n",
    "\n",
    "print(\"A conversão foi concluída com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatena abas de uma planilha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Caminho para o arquivo Excel\n",
    "file_path = r'E:\\thiagoborba.me\\sosrs\\data_management\\data_importing\\temp\\todas_planilhas\\working\\Familias SESC Protasio Alves.xlsx'\n",
    "\n",
    "# Ler todas as abas do arquivo Excel\n",
    "# Isso cria um dicionário de DataFrames\n",
    "all_sheets = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "# Concatenar todos os DataFrames em um único DataFrame\n",
    "# Use ignore_index=True para redefinir os índices no DataFrame resultante\n",
    "combined_df = pd.concat(all_sheets.values(), ignore_index=True)\n",
    "\n",
    "# Criar um ExcelWriter para salvar o resultado\n",
    "writer = pd.ExcelWriter(r'E:\\thiagoborba.me\\sosrs\\data_management\\data_importing\\temp\\todas_planilhas\\working\\Familias SESC Protasio Alves_merged.xlsx', engine='openpyxl')\n",
    "\n",
    "# Escrever o DataFrame concatenado em uma nova aba no arquivo Excel\n",
    "combined_df.to_excel(writer, sheet_name='Dados Concatenados')\n",
    "\n",
    "# Salvar e fechar o arquivo Excel\n",
    "writer.close()\n",
    "\n",
    "print(\"Todas as abas foram concatenadas com sucesso!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
