{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# (Legacy) Cargas de abrigados por template\n",
    "\n",
    "O template loader é uma carga realizada com base no template `abrigados_template_v1.xlsx`.\n",
    "\n",
    "> **Template file**\n",
    ">\n",
    "> abrigados\\template_loader\\templates\\abrigados_template_v1.xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "bootstrap"
    ],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalação de dependências\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "bootstrap"
    ]
   },
   "outputs": [],
   "source": [
    "## Importação de dependências\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import firestore\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict\n",
    "import uuid\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from selector import environment_selector\n",
    "from firestore_model_abrigado_legacy import AbrigadoLegacyEntity, MembroFamiliarLegacyEntity, DocumentoLegacyEntity, ResponsavelLegacyEntity\n",
    "from firestore_upload_data import upload_to_firestore, upload_to_firestore_checking_duplicated\n",
    "from firestore_query_batch import query_batch\n",
    "from nome import normalize_nome\n",
    "from datetime_xlsx import format_xlsx_datetime\n",
    "from documento import format_documento\n",
    "from additional_info import format_additional_info, sanitize_additional_info\n",
    "from abrigo_dict import get_abrigo_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "env-config"
    ]
   },
   "outputs": [],
   "source": [
    "environment_selector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "load-env"
    ]
   },
   "source": [
    "#### Configura chaves e parâmetros\n",
    "Carrega variáveis de ambiente e seta parâmetros como nome de collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "env-config"
    ]
   },
   "outputs": [],
   "source": [
    "# Firebase Firestore Config\n",
    "base_path = os.getenv(\"SOSRS_BASEPATH\")\n",
    "sosrs_environment = os.getenv(\"SOSRS_ENVIRONMENT\")\n",
    "sosrs_firestore_keyfile = os.getenv(\"SOSRS_FIRESTORE_KEYFILE\")\n",
    "sosrs_firestore_path = os.path.join(base_path, sosrs_firestore_keyfile)\n",
    "\n",
    "# Firestore Collection\n",
    "collection_name = \"abrigado\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autenticação do notebook\n",
    "Inicializa comunicação com Firestore. É necessário selecionar o ambiente no Environment Selector e ter configurado no `.env` os caminhos para o arquivo do `service-account do GCP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "env-config"
    ]
   },
   "outputs": [],
   "source": [
    "# Inicialização do Firebase\n",
    "cred = credentials.Certificate(sosrs_firestore_path)\n",
    "firebase_admin.initialize_app(cred)\n",
    "clientFirestore = firestore.client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template loader - Carregamento de abrigados baseado em template\n",
    "Utiliza um template acordado no AbrigosRS para realizar a carga de dados de diferentes fontes.\n",
    "Os fluxos tratados nesse notebook são:\n",
    "- Importação do template legado import_abrigados_template_v1.xlsx com retrocompatibilidade com o modelo atual da collection no Firestore.\n",
    "- Importação do template import_abrigados_template_v2.googlesheets.md com uma modelagem normalizada e baseada nos dados do templates, trazendo mais riqueza, qualidade e sentido para os dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nomes de colunas\n",
    "Nomeia as colunas do template para facilitar a leitura e entendimento no notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Define o nome das colunas do arquivo de entrada\n",
    "column_names = [\n",
    "    \"timestamp\",\n",
    "    \"abrigo\",\n",
    "    \"data_entrada\",\n",
    "    \"responsavel_nome\",\n",
    "    \"responsavel_nome_mae\",\n",
    "    \"responsavel_data_nascimento\",\n",
    "    \"responsavel_rg\",\n",
    "    \"responsavel_cpf\",\n",
    "    \"responsavel_nis\",\n",
    "    \"responsavel_titulo_eleitor\",\n",
    "    \"responsavel_outros_documentos\",\n",
    "    \"telefone\",\n",
    "    \"endereco\",\n",
    "    \"bairro\",\n",
    "    \"cidade\",\n",
    "    \"beneficio_informacoes\",\n",
    "    \"pessoa1_nome\",\n",
    "    \"pessoa1_parentesco\",\n",
    "    \"pessoa1_data_nascimento\",\n",
    "    \"pessoa2_nome\",\n",
    "    \"pessoa2_parentesco\",\n",
    "    \"pessoa2_data_nascimento\",\n",
    "    \"pessoa3_nome\",\n",
    "    \"pessoa3_parentesco\",\n",
    "    \"pessoa3_data_nascimento\",\n",
    "    \"pessoa4_nome\",\n",
    "    \"pessoa4_parentesco\",\n",
    "    \"pessoa4_data_nascimento\",\n",
    "    \"pessoa5_nome\",\n",
    "    \"pessoa5_parentesco\",\n",
    "    \"pessoa5_data_nascimento\",\n",
    "    \"pessoa6_nome\",\n",
    "    \"pessoa6_parentesco\",\n",
    "    \"pessoa6_data_nascimento\",\n",
    "    \"pessoa7_nome\",\n",
    "    \"pessoa7_parentesco\",\n",
    "    \"pessoa7_data_nascimento\",\n",
    "    \"pessoa8_nome\",\n",
    "    \"pessoa8_parentesco\",\n",
    "    \"pessoa8_data_nascimento\",\n",
    "    \"pessoa9_nome\",\n",
    "    \"pessoa9_parentesco\",\n",
    "    \"pessoa9_data_nascimento\",\n",
    "    \"pessoa10_nome\",\n",
    "    \"pessoa10_parentesco\",\n",
    "    \"pessoa10_data_nascimento\",\n",
    "    \"redes_de_apoio_informacoes\",\n",
    "    \"responsavel_preenchimento\",\n",
    "    \"informacoes_adicionais\",\n",
    "]\n",
    "\n",
    "# Define os campos do tipo data que devem ser tratados\n",
    "datetime_fields = [\n",
    "    \"timestamp\",\n",
    "    \"data_entrada\",\n",
    "    \"responsavel_data_nascimento\",\n",
    "    \"pessoa1_data_nascimento\",\n",
    "    \"pessoa2_data_nascimento\",\n",
    "    \"pessoa3_data_nascimento\",\n",
    "    \"pessoa4_data_nascimento\",\n",
    "    \"pessoa5_data_nascimento\",\n",
    "    \"pessoa6_data_nascimento\",\n",
    "    \"pessoa7_data_nascimento\",\n",
    "    \"pessoa8_data_nascimento\",\n",
    "    \"pessoa9_data_nascimento\",\n",
    "    \"pessoa10_data_nascimento\",\n",
    "]\n",
    "\n",
    "# Define campos manipulados no processamento\n",
    "search_field_name = \"search_field_name\"\n",
    "abrigo_nome = \"abrigo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração de datasources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Configuração do datasource\n",
    "datasource_file = \"./pipeline/.working/eeem_alcides_cunha_template_abrigados_abrigados.xlsx\"\n",
    "missing_abrigo = \"./pipeline/.working/missing_abrigo.json\"\n",
    "master_data_sheet = \"abrigados\"\n",
    "number_of_columns = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criação do dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "transformation-funcs"
    ]
   },
   "outputs": [],
   "source": [
    "# Cria uma entidade AbrigadoLegacyEntity a partir de uma linha do CSV\n",
    "\n",
    "\n",
    "def create_abrigado_entity(csv_row):\n",
    "\n",
    "    abrigo_nome, abrigo_id = get_abrigo_info(csv_row[\"abrigo\"])\n",
    "\n",
    "    # Se abrigo for vazio ou NaN, seta como 'abrigo nao informado na importacao'\n",
    "    if csv_row[\"abrigo\"] == \"\" or pd.isna(csv_row[\"abrigo\"]):\n",
    "        abrigo_nome, abrigo_id = get_abrigo_info(\"abrigo nao informado na importacao\")\n",
    "\n",
    "    abrigado_entity = AbrigadoLegacyEntity(\n",
    "        nome=csv_row[\"responsavel_nome\"],\n",
    "        search_field_name=normalize_nome(csv_row[\"responsavel_nome\"]),\n",
    "        dataNascimento=csv_row[\"responsavel_data_nascimento\"] if pd.notna(csv_row[\"responsavel_data_nascimento\"]) else None,\n",
    "        endereco=csv_row[\"endereco\"] if pd.notna(csv_row[\"endereco\"]) else None,\n",
    "        temDocumento=None,\n",
    "        acompanhadoMenor=None,\n",
    "        temRenda=None,\n",
    "        renda=None,\n",
    "        temHabitacao=None,\n",
    "        situacaoMoradia=None,\n",
    "        necessidadesImediatas=None,\n",
    "        cadastradoCadUnico=None,\n",
    "        abrigoId=abrigo_id,\n",
    "        abrigoNome=abrigo_nome,\n",
    "        documentos=[format_documento(csv_row[\"responsavel_cpf\"])],\n",
    "        additional_info=[\n",
    "            format_additional_info(\"data_entrada\", csv_row[\"data_entrada\"]),\n",
    "            format_additional_info(\"telefone\", csv_row[\"telefone\"]),\n",
    "            format_additional_info(\"bairro\", csv_row[\"bairro\"]),\n",
    "            format_additional_info(\"cidade\", csv_row[\"cidade\"]),\n",
    "            format_additional_info(\"beneficio_informacoes\", csv_row[\"beneficio_informacoes\"]),\n",
    "            format_additional_info(\"redes_de_apoio_informacoes\", csv_row[\"redes_de_apoio_informacoes\"]),\n",
    "            format_additional_info(\"responsavel_preenchimento\", csv_row[\"responsavel_preenchimento\"]),\n",
    "            format_additional_info(\"informacoes_adicionais\", csv_row[\"informacoes_adicionais\"]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Remove strings vazias\n",
    "    abrigado_entity.additional_info = sanitize_additional_info(abrigado_entity.additional_info)\n",
    "\n",
    "    return abrigado_entity\n",
    "\n",
    "\n",
    "def append_membro_familiar_entity(nome, data_nascimento, grau_parentesco, abrigado_entity):\n",
    "\n",
    "    # Criar um dicionário com valores válidos apenas\n",
    "    kwargs = {}\n",
    "    if pd.notna(nome) and str(nome).strip() != \"\":\n",
    "        kwargs[\"nome\"] = str(nome).strip()\n",
    "    if pd.notna(data_nascimento) and str(data_nascimento).strip() != \"\":\n",
    "        kwargs[\"data_nascimento\"] = str(data_nascimento).strip()\n",
    "    if pd.notna(grau_parentesco) and str(grau_parentesco).strip() != \"\":\n",
    "        kwargs[\"grau_parentesco\"] = str(grau_parentesco).strip()\n",
    "\n",
    "    # Verifica se há pelo menos um campo válido\n",
    "    if kwargs:\n",
    "        membro_familiar = MembroFamiliarLegacyEntity(**kwargs)\n",
    "\n",
    "        # Adiciona membro familiar à lista de membros familiares se existir a lista\n",
    "        if hasattr(abrigado_entity, \"grupoFamiliar\"):\n",
    "            if isinstance(abrigado_entity.grupoFamiliar, list):\n",
    "                abrigado_entity.grupoFamiliar.append(membro_familiar)\n",
    "            else:\n",
    "                abrigado_entity.grupoFamiliar = [membro_familiar]\n",
    "        else:\n",
    "            abrigado_entity.grupoFamiliar = [membro_familiar]\n",
    "    else:\n",
    "        print(\"Skipping...Nenhum dado válido para criar um membro familiar.\")\n",
    "\n",
    "    return abrigado_entity\n",
    "\n",
    "\n",
    "def append_responsavel_entity(nome, data_nascimento, abrigado_entity):\n",
    "\n",
    "    responsavel = ResponsavelLegacyEntity(\n",
    "        nome=nome if pd.notna(nome) and nome != \"\" else None,\n",
    "        data_nascimento=data_nascimento if pd.notna(data_nascimento) and data_nascimento != \"\" else None,\n",
    "        additional_info=None,\n",
    "    )\n",
    "\n",
    "    abrigado_entity.responsavel = responsavel\n",
    "\n",
    "    return abrigado_entity\n",
    "\n",
    "\n",
    "def append_documento_entity(cpf, rg, nis, titulo_eleitor, outros_documentos, abrigado_entity):\n",
    "    documento_entity = DocumentoLegacyEntity(\n",
    "        cpf=cpf if pd.notna(cpf) and cpf != \"\" else None,\n",
    "        rg=rg if pd.notna(rg) and rg != \"\" else None,\n",
    "        nis=nis if pd.notna(nis) and nis != \"\" else None,\n",
    "        titulo_eleitor=titulo_eleitor if pd.notna(titulo_eleitor) and titulo_eleitor != \"\" else None,\n",
    "        outros_documentos=outros_documentos if pd.notna(outros_documentos) else None,\n",
    "    )\n",
    "\n",
    "    abrigado_entity.responsavel.documentos = documento_entity\n",
    "\n",
    "    return abrigado_entity\n",
    "\n",
    "\n",
    "def transform_csv_data(csv_row):\n",
    "\n",
    "    # Cria entidade abrigado\n",
    "    abrigado_entity = create_abrigado_entity(csv_row)\n",
    "\n",
    "    # Adiciona membros familiares se existirem\n",
    "    for i in range(1, 11):\n",
    "        if i != 2:\n",
    "            append_membro_familiar_entity(\n",
    "                csv_row.get(f\"pessoa{i}_nome\"), csv_row.get(f\"pessoa{i}_parentesco\"), csv_row.get(f\"pessoa{i}_data_nascimento\"), abrigado_entity\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            append_membro_familiar_entity(\n",
    "                csv_row.get(f\"pessoa{i}_nome\"), csv_row.get(f\"pessoa{i}_data_nascimento\"), csv_row.get(f\"pessoa{i}_parentesco\"), abrigado_entity\n",
    "            )\n",
    "\n",
    "    # Adiciona responsável\n",
    "    append_responsavel_entity(csv_row[\"responsavel_nome\"], csv_row[\"responsavel_data_nascimento\"], abrigado_entity)\n",
    "\n",
    "    # Adiciona documentos\n",
    "    append_documento_entity(\n",
    "        csv_row[\"responsavel_cpf\"],\n",
    "        csv_row[\"responsavel_rg\"],\n",
    "        csv_row[\"responsavel_nis\"],\n",
    "        csv_row[\"responsavel_titulo_eleitor\"],\n",
    "        csv_row[\"responsavel_outros_documentos\"],\n",
    "        abrigado_entity,\n",
    "    )\n",
    "\n",
    "    return abrigado_entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Função de manipulação de dados duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "transformation-funcs"
    ]
   },
   "outputs": [],
   "source": [
    "def filter_duplicated_names(df_for_filtering):\n",
    "    # Consulta nomes\n",
    "    duplicity_check_result = query_batch(clientFirestore, collection_name, df_for_filtering, search_field_name, \"in\", 10)\n",
    "\n",
    "    # Filtrar para manter apenas entradas com um ID não None\n",
    "    registered_names = {name: id for name, id in duplicity_check_result.items() if id is not None}\n",
    "\n",
    "    # Lista de nomes já registrados\n",
    "    registered_names_list = list(registered_names.keys())\n",
    "\n",
    "    # Filtrar o DataFrame para remover nomes já registrados\n",
    "    df_filtered = df_for_filtering[~df_for_filtering[search_field_name].isin(registered_names_list)]\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução\n",
    "\n",
    "#### Abrigos não mapeados\n",
    "Para facilitar o processo de integração e não ficar impacto pela falta de normalização do nome dos abrigos, existe um mapeamento de abrigos entre o que estão nas planilhas e o que está cadastrado no sistema.\n",
    "Durante a importação, quando um abrigo não é encontrado no mapeamento, então ele é excluído do dataframe de processamento e é persistido em \"./datasource/recurrent/fasc/incoming/missing_abrigo.json\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Persiste nomes elegíveis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o arquivo xlsx agregador\n",
    "if os.path.exists(datasource_file):\n",
    "    xlsx = pd.ExcelFile(datasource_file)\n",
    "else:\n",
    "    print(f\"Datasource file not found. {datasource_file}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Define memória para abrigos não encontrados\n",
    "missing_abrigos = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    # Faz a leitura da aba master_data_sheet\n",
    "    data_source = pd.read_excel(xlsx, sheet_name=master_data_sheet, header=0, names=column_names, usecols=range(number_of_columns))\n",
    "\n",
    "    # Remove linhas onde a coluna 'nome' é NaN ou vazia\n",
    "    data_source = data_source[data_source[\"responsavel_nome\"].notna() & (data_source[\"responsavel_nome\"] != \"\")]\n",
    "\n",
    "    # Preenche campos vazios com strings vazias e formata campos de data\n",
    "    data_source.fillna(\"\", inplace=True)\n",
    "    format_xlsx_datetime(data_source, datetime_fields)\n",
    "\n",
    "    # Define as colunas para verificar duplicatas\n",
    "    duplicate_columns = [\n",
    "        \"responsavel_nome\",\n",
    "        \"responsavel_nome_mae\",\n",
    "        \"responsavel_data_nascimento\",\n",
    "        \"responsavel_rg\",\n",
    "        \"responsavel_cpf\",\n",
    "        \"responsavel_nis\",\n",
    "    ]\n",
    "\n",
    "    # Identifica duplicatas, mantendo a primeira ocorrência\n",
    "    duplicates = data_source[data_source.duplicated(subset=duplicate_columns, keep=False)]\n",
    "\n",
    "    # Filtra para manter apenas as duplicatas excluídas (ignora a primeira ocorrência)\n",
    "    duplicates_excluded = duplicates.drop_duplicates(subset=duplicate_columns, keep=\"first\")\n",
    "\n",
    "    # Remove linhas duplicadas baseado nas colunas especificadas no DataFrame original\n",
    "    data_source = data_source.drop_duplicates(subset=duplicate_columns, keep=\"first\")\n",
    "\n",
    "    if not duplicates_excluded.empty:\n",
    "        print(\"Registros duplicados que foram excluídos:\")\n",
    "        print(duplicates_excluded)\n",
    "\n",
    "    # Verifica se 'abrigo' é vazia ou NaN e 'responsavel_nome' é válida; se sim, define 'abrigo' como \"sem abrigo\"\n",
    "    data_source.loc[\n",
    "        (data_source[\"abrigo\"].isna() | (data_source[\"abrigo\"] == \"\"))\n",
    "        & data_source[\"responsavel_nome\"].notna()\n",
    "        & (data_source[\"responsavel_nome\"] != \"\"),\n",
    "        \"abrigo\",\n",
    "    ] = \"abrigononaoinformadonaimportacao\"\n",
    "\n",
    "    # Salva as colunas de data temporariamente\n",
    "    datetime_columns_data = data_source[datetime_fields].copy()\n",
    "\n",
    "    # Adiciona uma coluna para verificar a existência do abrigo usando get_abrigo_info\n",
    "    data_source[\"abrigo_exists\"] = data_source[datetime_columns_data].apply(lambda x: get_abrigo_info(x)[1] is not None)\n",
    "\n",
    "    # Define colunas para comparação de duplicatas, mantendo todas as colunas originais, exceto a coluna de busca específica\n",
    "    comparison_columns = [col for col in data_source.columns if col not in datetime_fields and col != \"search_field_name\"]\n",
    "\n",
    "    # Identifica registros duplicados usando as colunas definidas para comparação\n",
    "    data_source[\"duplicate\"] = data_source.duplicated(subset=comparison_columns, keep=False)\n",
    "\n",
    "    # Filtra para encontrar registros válidos: não duplicados e com abrigo existente\n",
    "    valid_dataframe = data_source[(data_source[\"abrigo_exists\"]) & (~data_source[\"duplicate\"])]\n",
    "\n",
    "    # Identifica registros com abrigos ausentes\n",
    "    missing_abrigos = data_source[~data_source[\"abrigo_exists\"]]\n",
    "\n",
    "    if not missing_abrigos.empty:\n",
    "        print(f\"Missing abrigos found: {missing_abrigos[\"search_field_name\"].unique()}\")\n",
    "        print(\"Removing missing abrigos from new records.\")\n",
    "        # Salvar os abrigos ausentes para arquivo JSON (substitua 'missing_abrigo' pela variável correta com o caminho do arquivo)\n",
    "        missing_abrigos.to_json(missing_abrigo, orient=\"records\", lines=True)\n",
    "\n",
    "        if valid_dataframe.empty:\n",
    "            print(\"All records have missing abrigos. Exiting...\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    # Atualiza data_source para conter apenas registros válidos\n",
    "    data_source = valid_dataframe.copy()\n",
    "\n",
    "    # Opcionalmente, pode-se remover as colunas 'abrigo_exists' e 'duplicate' se não forem mais necessárias\n",
    "    data_source.drop(columns=[\"abrigo_exists\", \"duplicate\"], inplace=True)\n",
    "\n",
    "    # Cria coluna search_field_nome\n",
    "    data_source[\"search_field_name\"] = data_source.apply(\n",
    "        lambda row: normalize_nome(row[\"search_field_name\"]), axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "except ValueError:\n",
    "    print(f\"Error reading sheets. Check the aggreate spreadsheet in the documentation. {master_data_sheet}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# Transforma os dados, valida e faz o upload\n",
    "try:\n",
    "    upload_to_firestore_checking_duplicated(clientFirestore, collection_name, data_source, transform_csv_data, filter_duplicated_names)\n",
    "    print(\"Upload finished.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading data to Firestore: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Files processing finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
